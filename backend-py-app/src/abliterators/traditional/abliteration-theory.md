# Mathematical Framework for Abliterator Systems

## 1. Common Foundations

### 1.1 Model Space Definition
Let M be the space of transformer model parameters where:
- Î¸ âˆˆ M represents model parameters
- H âŠ‚ â„áµˆ represents hidden state space with dimension d
- f_Î¸: X â†’ Y represents the model function

### 1.2 Activation Space
For layer l:
- A_l âŠ‚ â„áµˆ represents activation space
- h_l(x) âˆˆ A_l represents activations for input x
- W_l âˆˆ â„áµˆË£áµˆ represents weight matrices

## 2. Base Abliterator Framework

### 2.1 Harmful vs. Harmless Directions
For harmful activations H and harmless activations B:

```
Î£H = 1/n Î£áµ¢ háµ¢háµ¢áµ€    (harmful covariance)
Î£B = 1/n Î£áµ¢ báµ¢báµ¢áµ€    (harmless covariance)
```

### 2.2 Refusal Direction Calculation
The refusal direction r is computed as:

```
r = Î¼H - Î¼B
rÌ‚ = r/â€–râ€–â‚‚       (normalized refusal direction)
```

where:
- Î¼H = 1/n Î£áµ¢ háµ¢ (harmful mean)
- Î¼B = 1/n Î£áµ¢ báµ¢ (harmless mean)

### 2.3 Layer Modification
For each layer l:

```
W'_l = W_l - Î±P_r(W_l)
```

where:
- P_r(W) = (WrÌ‚)rÌ‚áµ€ is the projection onto r
- Î± is the ablation strength

### 2.4 Scoring Function
For token probabilities p:

```
s(x) = max(p(t|x): t âˆˆ T_neg)
```

where T_neg is the set of negative tokens.

## 3. Reverse Abliterator Framework

### 3.1 Target vs. Baseline Directions
For target activations T and baseline activations B:

```
Î£T = 1/n Î£áµ¢ táµ¢táµ¢áµ€    (target covariance)
Î£B = 1/n Î£áµ¢ báµ¢báµ¢áµ€    (baseline covariance)
```

### 3.2 Enhancement Direction Calculation
The enhancement direction e is:

```
e = Î¼T - Î¼B
Ãª = e/â€–eâ€–â‚‚       (normalized enhancement direction)
```

where:
- Î¼T = 1/n Î£áµ¢ táµ¢ (target mean)
- Î¼B = 1/n Î£áµ¢ báµ¢ (baseline mean)

### 3.3 Layer Enhancement
For each layer l:

```
W'_l = W_l + Î²P_e(W_l)
```

where:
- P_e(W) = (WÃª)Ãªáµ€ is the projection onto e
- Î² is the enhancement strength

### 3.4 Enhancement Score
For token probabilities p:

```
s(x) = max(p(t|x): t âˆˆ T_pos)
```

where T_pos is the set of target tokens.

## 4. Key Differences

### 4.1 Objective Functions

Base Abliterator:
```
min_Î¸ ð”¼â‚“âˆ¼X[max p(t|x) for t âˆˆ T_neg]
```

Reverse Abliterator:
```
max_Î¸ ð”¼â‚“âˆ¼X[max p(t|x) for t âˆˆ T_pos]
```

### 4.2 Modification Direction
Base: Subtractive modification
```
Î”W = -Î±P_r(W)
```

Reverse: Additive modification
```
Î”W = +Î²P_e(W)
```

## 5. Optimization Theory

### 5.1 Gradient Perspective
For both systems, the effective gradient is:

```
âˆ‡W = Â±Î³P_d(W)
```

where:
- Î³ is the learning rate (Î± or Î²)
- d is the direction (r or e)
- Â± depends on ablation vs. enhancement

### 5.2 Convergence Conditions
For stable convergence:

```
â€–W'_l - W_lâ€–_F â‰¤ Îµâ€–W_lâ€–_F
```

where:
- â€–Â·â€–_F is the Frobenius norm
- Îµ is the stability threshold

## 6. Performance Metrics

### 6.1 Directional Alignment
For direction d and activations a:

```
alignment(d,a) = |d^âŠ¤a|/(â€–dâ€–â‚‚â€–aâ€–â‚‚)
```

### 6.2 Layer-wise Effect
For layer l:

```
effect(l) = â€–P_d(W_l)â€–_F/â€–W_lâ€–_F
```

## 7. Implementation Considerations

### 7.1 Numerical Stability
Add regularization to covariance matrices:

```
Î£' = Î£ + Î»I
```

where Î» is a small positive constant.

### 7.2 Batch Processing
For batch size b:

```
Î¼_batch = 1/b Î£áµ¢â‚Œâ‚áµ‡ aáµ¢
```

## 8. Theoretical Guarantees

### 8.1 Direction Quality
For good direction vectors:

```
cos(d,Î¼_target) > cos(d,Î¼_other) + Î´
```

where Î´ is the separation margin.

### 8.2 Modification Bounds
For stable modifications:

```
â€–W'â€–_F â‰¤ (1 + Î³)â€–Wâ€–_F
```

where Î³ is the strength parameter.

## 9. Complexity Analysis

Space complexity: O(dÂ² + nd)
Time complexity: O(dÂ³ + ndk)

where:
- d is hidden state dimension
- n is number of samples
- k is number of top directions
